{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzRXFkjMSX-_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import shapiro\n",
        "from pacal import NormalDistr,UniformDistr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST Dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 5\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
      ],
      "metadata": {
        "id": "MjqdJSxww1Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definitions"
      ],
      "metadata": {
        "id": "TpcHsNcSw8wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the networks and the Train function"
      ],
      "metadata": {
        "id": "EHsuQmWwwb1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# network used for investigating neurons strength\n",
        "class LinearNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 200)\n",
        "        self.fc2 = nn.Linear(200, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        self.fc1data = self.fc1(x)\n",
        "        self.relufc1data = F.relu(self.fc1data)\n",
        "        self.fc2data = self.fc2(self.relufc1data)\n",
        "        self.relufc2data = F.relu(self.fc2data)\n",
        "        return self.fc2data       \n",
        "\n",
        "# Class for dense 2-layer networks with sigmoid as activation, \n",
        "# for mnist or other datasets with input of size 784 and 10 classes\n",
        "\n",
        "class NetSigmoid2(nn.Module):  \n",
        "    \n",
        "    def __init__(self, layer1, layer2):\n",
        "    #    layer1: number of nodes in the first hidden layer\n",
        "    #    layer2: number of nodes in second hidden layer\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, layer1)\n",
        "        self.fc2 = nn.Linear(layer1, layer2)\n",
        "        self.fc3 = nn.Linear(layer2, 10)        \n",
        "        \n",
        "    def forward(self, x):\n",
        "    #   x: input data\n",
        "    #   return: output of network\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = F.sigmoid(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "#Function for training networks, using cross entropy loss\n",
        "def Train(net, epochs, lr, trainloader, optimizerChoice = \"SGD\"):\n",
        "    # net: the net to be trained\n",
        "    # epochs: number of epochs to train for\n",
        "    # lr: learning rate/step size for the optimizer\n",
        "    # trainloader: loader for training data\n",
        "    # optimizerChoice = name of optimizer, \"SGD\" (default) or \"Adam\"\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    if optimizerChoice == \"SGD\":\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    elif optimizerChoice == \"Adam\":\n",
        "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    else:\n",
        "        raise Exception('Only \"SGD\" and \"Adam\" are supported as optimizerChoice')\n",
        "        \n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "e8RPjCm2wthH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This netwok corresponds to the network *net_final.pth*. \n",
        "The following code shows how we defined the network and how we trained the networ. It is a very simple network containing an input layer with 28*28 input neurons, one hidden layer with 200 nodes and one output layer with 10 nodes.  "
      ],
      "metadata": {
        "id": "jcFbgdsAwYbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = LinearNet()\n",
        "\n",
        "Train(net,3,0.01,trainloader=trainloader)"
      ],
      "metadata": {
        "id": "YiMaw8nVw4WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model \n",
        "PATH = \"C:/Users/Catalina/Documents/Bioinformatik/Master/2022_23_WS_Uppsala/Project/net_final.pth\"\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "7Nmu6I24xy5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "PATH = \"C:/Users/Catalina/Documents/Bioinformatik/Master/2022_23_WS_Uppsala/Project/net_final.pth\"\n",
        "net = LinearNet()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "l5EOsv7LxzCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define dense networks with two hidden layers, where the second layer contains 100 nodes for all networks, but the first layer has 2, 10, 100, 1000, or 10000 nodes. Of each type we define and save 10 untrained networks, then train them with sgd and adam as optimizers, to later be able to compare the results depending on optimizer. Code by Lovisa"
      ],
      "metadata": {
        "id": "YPY33BWWJbxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [2, 10, 100, 1000, 10000]\n",
        "layer2 = 100\n",
        "epochs = 3\n",
        "lrSGD = 0.01\n",
        "lrAdam = 0.001\n",
        "for i in sizes:\n",
        "    for k in range(10):\n",
        "        print(f'network {k} of size {i}')\n",
        "        net = NetSigmoid2(i, layer2)\n",
        "        torch.save(net.state_dict(), f'C:\\\\Users\\\\lovis\\\\OneDrive\\\\Dokument\\\\SavedNetworks\\\\Untrained{i}_{layer2}_number{k}')\n",
        "        Train(net, epochs, lrSGD, trainloader, \"SGD\")\n",
        "        torch.save(net.state_dict(), f'C:\\\\Users\\\\lovis\\\\OneDrive\\\\Dokument\\\\SavedNetworks\\\\SGDtrained{i}_{layer2}_number{k}')\n",
        "        net = NetSigmoid2(i, layer2)\n",
        "        net.load_state_dict(torch.load(f'C:\\\\Users\\\\lovis\\\\OneDrive\\\\Dokument\\\\SavedNetworks\\\\Untrained{i}_{layer2}_number{k}'))\n",
        "        net.eval()\n",
        "        Train(net, epochs, lrAdam, trainloader, \"Adam\")\n",
        "        torch.save(net.state_dict(), f'C:\\\\Users\\\\lovis\\\\OneDrive\\\\Dokument\\\\SavedNetworks\\\\Adamtrained{i}_{layer2}_number{k}')"
      ],
      "metadata": {
        "id": "zu1QPec0KGpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here starts the training of networks with varying learning rate"
      ],
      "metadata": {
        "id": "hKwXMA_UifIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate accuracy\n",
        "def evaluate_model(network):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(testloader,0):\n",
        "      inputs, labels = data\n",
        "      outputs = network(inputs)\n",
        "      for i,e in enumerate(outputs):\n",
        "        if torch.argmax(e) == labels[i]:\n",
        "          correct += 1\n",
        "        total += 1\n",
        "  return correct/total\n",
        "\n",
        "#finds the node strength of each node in the first layer\n",
        "def get_node_strength(network):\n",
        "  using_bias = network.fc1.bias != None\n",
        "  if using_bias:\n",
        "    bias_data = network.fc1.bias\n",
        "\n",
        "  weight_data = network.fc1.weight.data  \n",
        "  next_weight_data = network.fc2.weight.data  \n",
        "  num_nodes = weight_data.size(dim=0) #number of nodes in that layer\n",
        "  node_strength = np.zeros(num_nodes)  #empty numpy list\n",
        "\n",
        "  for i in range(num_nodes): #for each node\n",
        "    node_strength[i] += sum(weight_data[i,:]) #find incoming weights\n",
        "    if using_bias:\n",
        "      node_strength[i] += bias_data[i]*weight_data.size(dim=1) #find biases\n",
        "    node_strength[i] += sum(next_weight_data[:,i]) #find outgoing weights\n",
        "  return node_strength"
      ],
      "metadata": {
        "id": "UEUBMi6XjSPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training many networks to with random learning rate ADAM RELU\n",
        "num_networks = 100\n",
        "lrs = np.linspace(0.0001,0.01,num_networks) #varying learning rate\n",
        "for i in range(num_networks):\n",
        "  net = LinearNet()\n",
        "  lr = lrs[i]\n",
        "  Train(net,3,lr,trainloader,\"Adam\")\n",
        "  dt_string = datetime.now().strftime(\"%H%M%S\")\n",
        "  torch.save(net.state_dict(), f\"NetworkProject/lrAnalysis/relu_adam/{dt_string}\")\n",
        "  node_str = get_node_strength(net)\n",
        "  shap = shapiro(node_str)\n",
        "  with open('NetworkProject/lrAnalysis/relu_adam/learning_rate_relu_adam_data.csv', 'a',newline=\"\") as f:\n",
        "    write = csv.writer(f,delimiter=';')\n",
        "    write.writerow([i,lr,evaluate_model(net),shap[0],shap[1]])"
      ],
      "metadata": {
        "id": "RTL2jynwiqnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the initial distribution of node strength"
      ],
      "metadata": {
        "id": "9VfFsC9p526p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n1 = 784\n",
        "n2 = 10\n",
        "n = 20000\n",
        "randomnumbers = []\n",
        "for i in range(n):\n",
        "  summa = 0\n",
        "  w1 = np.random.uniform(-1/np.sqrt(n1),1/np.sqrt(n1),size=n1)\n",
        "  beta = np.random.uniform(-np.sqrt(n1),np.sqrt(n1))\n",
        "  w2 = np.random.uniform(-1/np.sqrt(n2),1/np.sqrt(n2),size=n2)\n",
        "\n",
        "  summa += sum(w1)+beta+sum(w2)\n",
        "  randomnumbers.append(summa)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "net = LinearNet(num_nodes = n)\n",
        "node_strength = get_node_strength(net)\n",
        "\n",
        "plt.hist(node_strength, bins=15, density=True, alpha=0.6, color='g')\n",
        "\n",
        "mu = np.mean(randomnumbers)\n",
        "sigma = np.sqrt(np.var(randomnumbers))\n",
        "print(mu,sigma**2)\n",
        "N = NormalDistr(0,1/3)\n",
        "M = NormalDistr(0,1/3)\n",
        "U = UniformDistr(-np.sqrt(n1),np.sqrt(n1))\n",
        "Y = N+U+M\n",
        "Y.plot()\n",
        "plt.xlabel(\"Node strength\")\n",
        "plt.ylabel(\"Probability density\")\n",
        "plt.title(f\"Probabiliy density of node strength for a single hidden layer\\n in an untrained network with {n} nodes and {n1} input variables.\\n Mean = {mu:.2f}, std = {sigma:.2f}\")\n",
        "plt.legend([\"Observed\",\"Theoretical\"])\n",
        "plt.savefig(\"initialdistribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "13a4tqF757cw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}